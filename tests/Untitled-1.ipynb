{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(name='tiny-llama-1.1b', hf_config={'org': 'TinyLlama', 'name': 'TinyLlama-1.1B-intermediate-step-1431k-3T'}, scale_embeddings=False, block_size=2048, vocab_size=32000, padding_multiple=64, padded_vocab_size=32000, n_layer=22, n_head=32, head_size=64, n_embd=2048, rotary_percentage=1.0, parallel_residual=False, bias=False, lm_head_bias=False, n_query_groups=4, shared_attention_norm=False, norm_class_name='RMSNorm', norm_eps=1e-05, mlp_class_name='LLaMAMLP', gelu_approximate='none', intermediate_size=5632, rope_condense_ratio=1, rope_base=10000, n_expert=0, n_expert_per_token=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal, Optional, Type, Union\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from typing_extensions import Self\n",
    "\n",
    "import litgpt.model\n",
    "from litgpt.utils import find_multiple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    name: str = \"\"\n",
    "    hf_config: dict = field(default_factory=dict)\n",
    "    scale_embeddings: bool = False\n",
    "    block_size: int = 4096\n",
    "    vocab_size: int = 50254\n",
    "    padding_multiple: int = 512\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 16\n",
    "    n_head: int = 32\n",
    "    head_size: Optional[int] = None\n",
    "    n_embd: int = 4096\n",
    "    rotary_percentage: float = 0.25\n",
    "    parallel_residual: bool = True\n",
    "    bias: bool = True\n",
    "    lm_head_bias: bool = False\n",
    "    # to use multi-head attention (MHA), set this to `n_head` (default)\n",
    "    # to use multi-query attention (MQA), set this to 1\n",
    "    # to use grouped-query attention (GQA), set this to a value in between\n",
    "    # Example with `n_head=4`\n",
    "    # ┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
    "    # │ v ││ v ││ v ││ v │     │ v │    │ v │             │ v │\n",
    "    # └───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
    "    #   │    │    │    │         │        │                 │\n",
    "    # ┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
    "    # │ k ││ k ││ k ││ k │     │ k │    │ k │             │ k │\n",
    "    # └───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
    "    #   │    │    │    │      ┌──┴──┐  ┌──┴──┐      ┌────┬──┴─┬────┐\n",
    "    # ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐\n",
    "    # │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │\n",
    "    # └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘\n",
    "    # ◀──────────────────▶  ◀──────────────────▶  ◀──────────────────▶\n",
    "    #         MHA                    GQA                   MQA\n",
    "    #   n_query_groups=4       n_query_groups=2      n_query_groups=1\n",
    "    #\n",
    "    # credit https://arxiv.org/pdf/2305.13245.pdf\n",
    "    n_query_groups: Optional[int] = None\n",
    "    shared_attention_norm: bool = False\n",
    "    norm_class_name: Literal[\"LayerNorm\", \"RMSNorm\"] = \"LayerNorm\"\n",
    "    norm_eps: float = 1e-5\n",
    "    mlp_class_name: Literal[\"GptNeoxMLP\", \"LLaMAMLP\", \"GemmaMLP\", \"LLaMAMoE\"] = \"GptNeoxMLP\"\n",
    "    gelu_approximate: str = \"none\"\n",
    "    intermediate_size: Optional[int] = None\n",
    "    rope_condense_ratio: int = 1\n",
    "    rope_base: int = 10000\n",
    "    n_expert: int = 0\n",
    "    n_expert_per_token: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.name:\n",
    "            self.name = self.hf_config.get(\"name\", self.name)\n",
    "\n",
    "        if self.head_size is None:\n",
    "            assert self.n_embd % self.n_head == 0\n",
    "            self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        # vocab size should be a power of 2 to be optimal on hardware. compute the closest value\n",
    "        if self.padded_vocab_size is None:\n",
    "            self.padded_vocab_size = find_multiple(self.vocab_size, self.padding_multiple)\n",
    "        else:\n",
    "            # vocab size shouldn't be larger than padded vocab size\n",
    "            self.vocab_size = min(self.vocab_size, self.padded_vocab_size)\n",
    "\n",
    "        # compute the number of query groups\n",
    "        if self.n_query_groups is not None:\n",
    "            assert self.n_head % self.n_query_groups == 0\n",
    "        else:\n",
    "            self.n_query_groups = self.n_head\n",
    "\n",
    "        # compute the intermediate size for MLP if not set\n",
    "        if self.intermediate_size is None:\n",
    "            if self.mlp_class_name == \"LLaMAMLP\":\n",
    "                raise ValueError(f\"The config {self.name!r}, needs to set the `intermediate_size`\")\n",
    "            self.intermediate_size = 4 * self.n_embd\n",
    "\n",
    "        self.rope_n_elem = int(self.rotary_percentage * self.head_size)\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str, **kwargs: Any) -> Optional[Self]:\n",
    "        if name not in name_to_config:\n",
    "            # search through all `config['hf_config']['name']`\n",
    "            try:\n",
    "                conf_dict = next(config for config in configs if name == config[\"hf_config\"][\"name\"])\n",
    "            \n",
    "            # try to get name from an org/model string like \"meta-llama/Meta-Llama-3-8B\"\n",
    "            except StopIteration:\n",
    "                try:\n",
    "                    conf_dict = next(config for config in configs if config[\"hf_config\"][\"org\"] + \"/\" + config[\"hf_config\"][\"name\"] == name)\n",
    "                except StopIteration:\n",
    "                    raise ValueError(f\"{name!r} is not a supported config name\")\n",
    "        else:\n",
    "             conf_dict = name_to_config[name]\n",
    "\n",
    "        conf_dict = conf_dict.copy()\n",
    "        conf_dict.update(kwargs)\n",
    "        return cls(**conf_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path: Union[str, Path], **kwargs: Any) -> Self:\n",
    "        with open(path, encoding=\"utf-8\") as fp:\n",
    "            file_kwargs = yaml.safe_load(fp)\n",
    "            if file_kwargs is None:\n",
    "                raise ValueError(f\"{path} is empty which is likely unexpected.\")\n",
    "        file_kwargs.update(kwargs)\n",
    "        return cls(**file_kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, path: Path, **kwargs: Any) -> Self:\n",
    "        \"\"\"Automatically load `model_config.yaml` and if it doesn't exist - a matching config from `litgpt/config.py`.\"\"\"\n",
    "        if (config_path := path / \"model_config.yaml\").is_file():\n",
    "            return cls.from_file(config_path, **kwargs)\n",
    "        if (model_name := path.name) in name_to_config:\n",
    "            return cls.from_name(model_name, **kwargs)\n",
    "        raise FileNotFoundError(f\"For {str(path)!r} neither 'model_config.yaml' nor matching config exists.\")\n",
    "\n",
    "    @property\n",
    "    def mlp_class(self) -> Type:\n",
    "        # `self.mlp_class_name` cannot be the type to keep the config serializable\n",
    "        return getattr(litgpt.model, self.mlp_class_name)\n",
    "\n",
    "    @property\n",
    "    def norm_class(self) -> Type:\n",
    "        # `self.norm_class_name` cannot be the type to keep the config serializable\n",
    "        if self.norm_class_name == \"RMSNorm\":\n",
    "            from functools import partial\n",
    "\n",
    "            from litgpt.model import RMSNorm\n",
    "\n",
    "            return partial(RMSNorm, add_unit_offset=\"Gemma\" in self.name)\n",
    "        return getattr(torch.nn, self.norm_class_name)\n",
    "\n",
    "\n",
    "########################\n",
    "# Stability AI StableLM\n",
    "########################\n",
    "configs = [\n",
    "    # https://huggingface.co/stabilityai/stablelm-base-alpha-3b/blob/main/config.json\n",
    "    dict(name=\"stablelm-base-alpha-3b\", hf_config=dict(org=\"stabilityai\", name=\"stablelm-base-alpha-3b\")),\n",
    "    # https://huggingface.co/stabilityai/stablelm-base-alpha-7b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"stablelm-base-alpha-7b\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"stablelm-base-alpha-7b\"),\n",
    "        n_head=48,\n",
    "        n_embd=6144,\n",
    "        padding_multiple=256,\n",
    "    ),\n",
    "    # https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/blob/main/config.json\n",
    "    dict(name=\"stablelm-tuned-alpha-3b\", hf_config=dict(org=\"stabilityai\", name=\"stablelm-tuned-alpha-3b\"), n_head=32),\n",
    "    # https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"stablelm-tuned-alpha-7b\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"stablelm-tuned-alpha-7b\"),\n",
    "        n_head=48,\n",
    "        n_embd=6144,\n",
    "        padding_multiple=256,\n",
    "    ),\n",
    "    # https://huggingface.co/stabilityai/stablelm-3b-4e1t/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"stablelm-3b-4e1t\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"stablelm-3b-4e1t\"),\n",
    "        padded_vocab_size=50304,\n",
    "        n_layer=32,\n",
    "        n_head=32,\n",
    "        n_embd=2560,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=6912,\n",
    "    ),\n",
    "    # https://huggingface.co/stabilityai/stablelm-zephyr-3b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"stablelm-zephyr-3b\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"stablelm-zephyr-3b\"),\n",
    "        padded_vocab_size=50304,\n",
    "        n_layer=32,\n",
    "        n_head=32,\n",
    "        n_embd=2560,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=6912,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "##########################\n",
    "# Stability AI StableCode\n",
    "##########################\n",
    "stablecode = [\n",
    "    # https://huggingface.co/stabilityai/stablecode-completion-alpha-3b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"stablecode-completion-alpha-3b\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"stablecode-completion-alpha-3b\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=49152,\n",
    "        n_layer=32,\n",
    "        n_embd=2560,\n",
    "    ),\n",
    "    # https://huggingface.co/stabilityai/stablecode-completion-alpha-3b-4k/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"stablecode-completion-alpha-3b-4k\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"stablecode-completion-alpha-3b-4k\"),\n",
    "        vocab_size=49152,\n",
    "        n_layer=32,\n",
    "        n_embd=2560,\n",
    "    ),\n",
    "    # https://huggingface.co/stabilityai/stablecode-instruct-alpha-3b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"stablecode-instruct-alpha-3b\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"stablecode-instruct-alpha-3b\"),\n",
    "        vocab_size=49152,\n",
    "        n_layer=32,\n",
    "        n_embd=2560,\n",
    "    ),\n",
    "    # https://huggingface.co/stabilityai/stable-code-3b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"stable-code-3b\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"stable-code-3b\"),\n",
    "        padded_vocab_size=50304,\n",
    "        n_layer=32,\n",
    "        n_embd=2560,\n",
    "        block_size=16384,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=6912,\n",
    "    ),\n",
    "]\n",
    "configs.extend(stablecode)\n",
    "\n",
    "\n",
    "####################\n",
    "# EleutherAI Pythia\n",
    "####################\n",
    "pythia = [\n",
    "    # https://huggingface.co/EleutherAI/pythia-14m/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-14m\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-14m\"),\n",
    "        block_size=512,\n",
    "        n_layer=6,\n",
    "        n_embd=128,\n",
    "        n_head=4,\n",
    "        padding_multiple=128,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-31m/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-31m\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-31m\"),\n",
    "        block_size=1024,\n",
    "        n_layer=6,\n",
    "        n_embd=256,\n",
    "        n_head=8,\n",
    "        padding_multiple=128,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-70m/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-70m\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-70m\"),\n",
    "        block_size=2048,\n",
    "        n_layer=6,\n",
    "        n_embd=512,\n",
    "        n_head=8,\n",
    "        padding_multiple=128,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-160m/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-160m\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-160m\"),\n",
    "        block_size=2048,\n",
    "        n_layer=12,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        padding_multiple=128,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-410m/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-410m\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-410m\"),\n",
    "        block_size=2048,\n",
    "        n_layer=24,\n",
    "        n_embd=1024,\n",
    "        n_head=16,\n",
    "        padding_multiple=128,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-1b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-1b\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-1b\"),\n",
    "        block_size=2048,\n",
    "        n_embd=2048,\n",
    "        n_head=8,\n",
    "        padding_multiple=128,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-1.4b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-1.4b\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-1.4b\"),\n",
    "        block_size=2048,\n",
    "        n_layer=24,\n",
    "        n_embd=2048,\n",
    "        n_head=16,\n",
    "        padding_multiple=128,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-2.8b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-2.8b\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-2.8b\"),\n",
    "        block_size=2048,\n",
    "        n_layer=32,\n",
    "        n_embd=2560,\n",
    "        padding_multiple=128,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-6.9b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-6.9b\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-6.9b\"),\n",
    "        block_size=2048,\n",
    "        n_layer=32,\n",
    "        padding_multiple=256,\n",
    "    ),\n",
    "    # https://huggingface.co/EleutherAI/pythia-12b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"pythia-12b\",\n",
    "        hf_config=dict(org=\"EleutherAI\", name=\"pythia-12b\"),\n",
    "        block_size=2048,\n",
    "        n_layer=36,\n",
    "        n_embd=5120,\n",
    "        n_head=40,\n",
    "    ),\n",
    "]\n",
    "configs.extend(pythia)\n",
    "for c in pythia:\n",
    "    # \"pythia-14m\" and \"pythia-31m\" don't have deduped version\n",
    "    if c[\"name\"] in (\"pythia-14m\", \"pythia-31m\"):\n",
    "        continue\n",
    "    copy = deepcopy(c)\n",
    "    copy[\"name\"] = f\"{c['name']}-deduped\"\n",
    "    copy[\"hf_config\"][\"name\"] = f\"{c['hf_config']['name']}-deduped\"\n",
    "    configs.append(copy)\n",
    "\n",
    "\n",
    "###################\n",
    "# databricks Dolly\n",
    "###################\n",
    "dolly = [\n",
    "    # https://huggingface.co/databricks/dolly-v2-3b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"dolly-v2-3b\",\n",
    "        hf_config=dict(org=\"databricks\", name=\"dolly-v2-3b\"),\n",
    "        block_size=2048,\n",
    "        n_layer=32,\n",
    "        n_embd=2560,\n",
    "        padded_vocab_size=50280,\n",
    "    ),\n",
    "    # https://huggingface.co/databricks/dolly-v2-7b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"dolly-v2-7b\",\n",
    "        hf_config=dict(org=\"databricks\", name=\"dolly-v2-7b\"),\n",
    "        block_size=2048,\n",
    "        n_layer=32,\n",
    "        padded_vocab_size=50280,\n",
    "    ),\n",
    "    # https://huggingface.co/databricks/dolly-v2-12b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"dolly-v2-12b\",\n",
    "        hf_config=dict(org=\"databricks\", name=\"dolly-v2-12b\"),\n",
    "        block_size=2048,\n",
    "        n_layer=36,\n",
    "        n_embd=5120,\n",
    "        n_head=40,\n",
    "        padded_vocab_size=50280,\n",
    "    ),\n",
    "]\n",
    "configs.extend(dolly)\n",
    "\n",
    "\n",
    "####################################\n",
    "# togethercomputer RedPajama INCITE\n",
    "####################################\n",
    "redpajama_incite = [\n",
    "    # https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"RedPajama-INCITE-{}-3B-v1\",\n",
    "        hf_config=dict(org=\"togethercomputer\", name=\"RedPajama-INCITE-{}-3B-v1\"),\n",
    "        block_size=2048,\n",
    "        n_layer=32,\n",
    "        n_embd=2560,\n",
    "        padding_multiple=256,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "    ),\n",
    "    # https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"RedPajama-INCITE-7B-{}\",\n",
    "        hf_config=dict(org=\"togethercomputer\", name=\"RedPajama-INCITE-7B-{}\"),\n",
    "        block_size=2048,\n",
    "        n_layer=32,\n",
    "        padding_multiple=256,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "    ),\n",
    "    # this redirects to the checkpoint above. kept for those who had the old weights already downloaded\n",
    "    dict(\n",
    "        name=\"RedPajama-INCITE-{}-7B-v0.1\",\n",
    "        hf_config=dict(org=\"togethercomputer\", name=\"RedPajama-INCITE-{}-7B-v0.1\"),\n",
    "        block_size=2048,\n",
    "        n_layer=32,\n",
    "        padding_multiple=256,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "    ),\n",
    "]\n",
    "for c in redpajama_incite:\n",
    "    for kind in (\"Base\", \"Chat\", \"Instruct\"):\n",
    "        copy = deepcopy(c)\n",
    "        copy[\"name\"] = c[\"name\"].format(kind)\n",
    "        copy[\"hf_config\"][\"name\"] = c[\"hf_config\"][\"name\"].format(kind)\n",
    "        configs.append(copy)\n",
    "\n",
    "\n",
    "#################\n",
    "# TII UAE Falcon\n",
    "#################\n",
    "falcon = [\n",
    "    # https://huggingface.co/tiiuae/falcon-7b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"falcon-7b{}\",\n",
    "        hf_config=dict(org=\"tiiuae\", name=\"falcon-7b{}\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=65024,\n",
    "        padded_vocab_size=65024,\n",
    "        n_layer=32,\n",
    "        n_head=71,\n",
    "        n_embd=4544,\n",
    "        rotary_percentage=1.0,\n",
    "        n_query_groups=1,\n",
    "        bias=False,\n",
    "        # this is not in the config, but in the original model implementation, only for this config\n",
    "        shared_attention_norm=True,\n",
    "    ),\n",
    "    # https://huggingface.co/tiiuae/falcon-40b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"falcon-40b{}\",\n",
    "        hf_config=dict(org=\"tiiuae\", name=\"falcon-40b{}\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=65024,\n",
    "        padded_vocab_size=65024,\n",
    "        n_layer=60,\n",
    "        n_head=128,\n",
    "        n_embd=8192,\n",
    "        rotary_percentage=1.0,\n",
    "        n_query_groups=8,\n",
    "        bias=False,\n",
    "    ),\n",
    "]\n",
    "for c in falcon:\n",
    "    for kind in (\"\", \"-instruct\"):\n",
    "        copy = deepcopy(c)\n",
    "        copy[\"name\"] = c[\"name\"].format(kind)\n",
    "        copy[\"hf_config\"][\"name\"] = c[\"hf_config\"][\"name\"].format(kind)\n",
    "        configs.append(copy)\n",
    "\n",
    "# https://huggingface.co/tiiuae/falcon-180b/blob/main/config.json\n",
    "falcon180b = dict(\n",
    "    name=\"falcon-180B{}\",\n",
    "    hf_config=dict(org=\"tiiuae\", name=\"falcon-180B{}\"),\n",
    "    block_size=2048,\n",
    "    vocab_size=65024,\n",
    "    padded_vocab_size=65024,\n",
    "    n_layer=80,\n",
    "    n_head=232,\n",
    "    n_embd=14848,\n",
    "    rotary_percentage=1.0,\n",
    "    n_query_groups=8,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "for kind in (\"\", \"-chat\"):\n",
    "    copy = deepcopy(falcon180b)\n",
    "    copy[\"name\"] = falcon180b[\"name\"].format(kind)\n",
    "    copy[\"hf_config\"][\"name\"] = falcon180b[\"hf_config\"][\"name\"].format(kind)\n",
    "    configs.append(copy)\n",
    "\n",
    "\n",
    "#############################\n",
    "# OpenLM Research Open LLaMA\n",
    "#############################\n",
    "open_LLaMA = [\n",
    "    # https://huggingface.co/openlm-research/open_llama_3b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"open_llama_3b\",\n",
    "        hf_config=dict(org=\"openlm-research\", name=\"open_llama_3b\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=26,\n",
    "        n_embd=3200,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=8640,\n",
    "    ),\n",
    "    # https://huggingface.co/openlm-research/open_llama_7b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"open_llama_7b\",\n",
    "        hf_config=dict(org=\"openlm-research\", name=\"open_llama_7b\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "    ),\n",
    "    # https://huggingface.co/openlm-research/open_llama_13b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"open_llama_13b\",\n",
    "        hf_config=dict(org=\"openlm-research\", name=\"open_llama_13b\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "]\n",
    "configs.extend(open_LLaMA)\n",
    "\n",
    "\n",
    "###############\n",
    "# LMSYS Vicuna\n",
    "###############\n",
    "vicuna = [\n",
    "    # https://huggingface.co/lmsys/vicuna-7b-v1.3/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"vicuna-7b-v1.3\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"vicuna-7b-v1.3\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "    ),\n",
    "    # https://huggingface.co/lmsys/vicuna-13b-v1.3/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"vicuna-13b-v1.3\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"vicuna-13b-v1.3\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "    # https://huggingface.co/lmsys/vicuna-33b-v1.3/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"vicuna-33b-v1.3\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"vicuna-33b-v1.3\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=60,\n",
    "        n_head=52,\n",
    "        n_embd=6656,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=17920,\n",
    "    ),\n",
    "    # https://huggingface.co/lmsys/vicuna-7b-v1.5/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"vicuna-7b-v1.5\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"vicuna-7b-v1.5\"),\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "    ),\n",
    "    # https://huggingface.co/lmsys/vicuna-7b-v1.5-16k/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"vicuna-7b-v1.5-16k\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"vicuna-7b-v1.5-16k\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "        rope_condense_ratio=4,\n",
    "    ),\n",
    "    # https://huggingface.co/lmsys/vicuna-13b-v1.5/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"vicuna-13b-v1.5\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"vicuna-13b-v1.5\"),\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "    # https://huggingface.co/lmsys/vicuna-13b-v1.5-16k/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"vicuna-13b-v1.5-16k\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"vicuna-13b-v1.5-16k\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "        rope_condense_ratio=4,\n",
    "    ),\n",
    "]\n",
    "configs.extend(vicuna)\n",
    "\n",
    "\n",
    "#################\n",
    "# LMSYS LongChat\n",
    "#################\n",
    "long_chat = [\n",
    "    # https://huggingface.co/lmsys/longchat-7b-16k/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"longchat-7b-16k\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"longchat-7b-16k\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "        rope_condense_ratio=8,\n",
    "    ),\n",
    "    # https://huggingface.co/lmsys/longchat-13b-16k/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"longchat-13b-16k\",\n",
    "        hf_config=dict(org=\"lmsys\", name=\"longchat-13b-16k\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "        rope_condense_ratio=8,\n",
    "    ),\n",
    "]\n",
    "configs.extend(long_chat)\n",
    "\n",
    "\n",
    "######################\n",
    "# NousResearch Hermes\n",
    "######################\n",
    "nous_research = [\n",
    "    # https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Nous-Hermes-llama-2-7b\",\n",
    "        hf_config=dict(org=\"NousResearch\", name=\"Nous-Hermes-llama-2-7b\"),\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "    ),\n",
    "    # https://huggingface.co/NousResearch/Nous-Hermes-13B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Nous-Hermes-13b\",\n",
    "        hf_config=dict(org=\"NousResearch\", name=\"Nous-Hermes-13b\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32000,\n",
    "        padded_vocab_size=32001,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-6,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "    # https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b\n",
    "    dict(\n",
    "        name=\"Nous-Hermes-Llama2-13b\",\n",
    "        hf_config=dict(org=\"NousResearch\", name=\"Nous-Hermes-Llama2-13b\"),\n",
    "        vocab_size=32000,\n",
    "        padded_vocab_size=32032,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "]\n",
    "configs.extend(nous_research)\n",
    "\n",
    "\n",
    "###############\n",
    "# Meta LLaMA 2\n",
    "###############\n",
    "llama_2 = [\n",
    "    # https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Llama-2-7b{}-hf\",\n",
    "        hf_config=dict(org=\"meta-llama\", name=\"Llama-2-7b{}-hf\"),\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "    ),\n",
    "    # https://huggingface.co/meta-llama/Llama-2-13b-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Llama-2-13b{}-hf\",\n",
    "        hf_config=dict(org=\"meta-llama\", name=\"Llama-2-13b{}-hf\"),\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "    # https://huggingface.co/meta-llama/Llama-2-70b-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Llama-2-70b{}-hf\",\n",
    "        hf_config=dict(org=\"meta-llama\", name=\"Llama-2-70b{}-hf\"),\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "    ),\n",
    "]\n",
    "for c in llama_2:\n",
    "    for kind in (\"\", \"-chat\"):\n",
    "        copy = deepcopy(c)\n",
    "        copy[\"name\"] = c[\"name\"].format(kind)\n",
    "        copy[\"hf_config\"][\"name\"] = c[\"hf_config\"][\"name\"].format(kind)\n",
    "        configs.append(copy)\n",
    "\n",
    "\n",
    "###############\n",
    "# Meta LLaMA 3\n",
    "###############\n",
    "llama_3 = [\n",
    "    # https://huggingface.co/meta-llama/Meta-Llama-3-8B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Llama-3-8B{}\",\n",
    "        hf_config=dict(org=\"meta-llama\", name=\"Meta-Llama-3-8B{}\"),\n",
    "        block_size=8192,\n",
    "        vocab_size=128000,\n",
    "        padded_vocab_size=128256,\n",
    "        n_layer=32,\n",
    "        n_head=32,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=14336,\n",
    "        rope_base=500000,\n",
    "    ),\n",
    "    # https://huggingface.co/meta-llama/Meta-Llama-3-70B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Llama-3-70B{}\",\n",
    "        hf_config=dict(org=\"meta-llama\", name=\"Meta-Llama-3-70B{}\"),\n",
    "        block_size=8192,\n",
    "        vocab_size=128000,\n",
    "        padded_vocab_size=128256,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "        rope_base=500000,\n",
    "    ),\n",
    "]\n",
    "for c in llama_3:\n",
    "    for kind in (\"\", \"-Instruct\"):\n",
    "        copy = deepcopy(c)\n",
    "        copy[\"name\"] = c[\"name\"].format(kind)\n",
    "        copy[\"hf_config\"][\"name\"] = c[\"hf_config\"][\"name\"].format(kind)\n",
    "        configs.append(copy)\n",
    "\n",
    "\n",
    "###############\n",
    "# Google Gemma\n",
    "###############\n",
    "gemma = [\n",
    "    # https://huggingface.co/google/gemma-2b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Gemma-2b\",\n",
    "        hf_config=dict(org=\"google\", name=\"gemma-2b\"),\n",
    "        scale_embeddings=True,\n",
    "        vocab_size=256000,\n",
    "        padding_multiple=64,\n",
    "        n_embd=2048,\n",
    "        n_layer=18,\n",
    "        n_head=8,\n",
    "        n_query_groups=1,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"GemmaMLP\",\n",
    "        gelu_approximate=\"tanh\",\n",
    "        intermediate_size=16384,\n",
    "    ),\n",
    "    # https://huggingface.co/google/gemma-7b/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Gemma-7b\",\n",
    "        hf_config=dict(org=\"google\", name=\"gemma-7b\"),\n",
    "        scale_embeddings=True,\n",
    "        vocab_size=256000,\n",
    "        padding_multiple=64,\n",
    "        n_embd=3072,\n",
    "        n_layer=28,\n",
    "        n_head=16,\n",
    "        head_size=256,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"GemmaMLP\",\n",
    "        gelu_approximate=\"tanh\",\n",
    "        intermediate_size=24576,\n",
    "    ),\n",
    "]\n",
    "configs.extend(gemma)\n",
    "for c in gemma:\n",
    "    copy = deepcopy(c)\n",
    "    copy[\"name\"] = f\"{c['name']}-it\"\n",
    "    copy[\"hf_config\"][\"name\"] = f\"{c['hf_config']['name']}-it\"\n",
    "    configs.append(copy)\n",
    "\n",
    "##################\n",
    "# Google CodeGemma\n",
    "##################\n",
    "codegemma = [\n",
    "    # https://huggingface.co/google/codegemma-7b-it/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeGemma-7b-it\",\n",
    "        hf_config=dict(org=\"google\", name=\"codegemma-7b-it\"),\n",
    "        scale_embeddings=True,\n",
    "        vocab_size=256000,\n",
    "        padding_multiple=64,\n",
    "        n_embd=3072,\n",
    "        n_layer=28,\n",
    "        n_head=16,\n",
    "        head_size=256,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"GemmaMLP\",\n",
    "        gelu_approximate=\"tanh\",\n",
    "        intermediate_size=24576,\n",
    "    ),\n",
    "]\n",
    "configs.extend(codegemma)\n",
    "\n",
    "################\n",
    "# H2Oai Danube2\n",
    "################\n",
    "danube2 = [\n",
    "    # https://huggingface.co/h2oai/h2o-danube2-1.8b-chat/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Danube2-1.8b-chat\",\n",
    "        hf_config=dict(org=\"h2oai\", name=\"h2o-danube2-1.8b-chat\"),\n",
    "        vocab_size=32000,\n",
    "        n_layer=24,\n",
    "        n_head=32,\n",
    "        n_embd=2560,\n",
    "        block_size=4096,  # should be 8192 but sliding_window mechanism is not implemented\n",
    "        intermediate_size=6912,\n",
    "        padding_multiple=64,\n",
    "        norm_eps=1e-05,\n",
    "        rope_base=10000,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "    )\n",
    "]\n",
    "configs.extend(danube2)\n",
    "\n",
    "\n",
    "##########################\n",
    "# Stability AI FreeWilly2\n",
    "##########################\n",
    "freewilly_2 = [\n",
    "    # https://huggingface.co/stabilityai/FreeWilly2/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"FreeWilly2\",\n",
    "        hf_config=dict(org=\"stabilityai\", name=\"FreeWilly2\"),\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "    )\n",
    "]\n",
    "configs.extend(freewilly_2)\n",
    "\n",
    "\n",
    "##################\n",
    "# Meta Code Llama\n",
    "##################\n",
    "code_llama = [\n",
    "    # https://huggingface.co/codellama/CodeLlama-7b-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-7b-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-7b-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32016,\n",
    "        padding_multiple=16,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-13b-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-13b-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-13b-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32016,\n",
    "        padding_multiple=16,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-34b-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-34b-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-34b-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=48,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=22016,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-70b-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-70b-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-70b-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32016,\n",
    "        padding_multiple=16,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-7b-Python-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-7b-Python-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-7b-Python-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-13b-Python-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-13b-Python-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-13b-Python-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-34b-Python-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-34b-Python-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-34b-Python-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=48,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=22016,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-70b-Python-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-70b-Python-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-70b-Python-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32016,\n",
    "        padding_multiple=16,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-7b-Instruct-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-7b-Instruct-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32016,\n",
    "        padding_multiple=16,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-13b-Instruct-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-13b-Instruct-hf\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32016,\n",
    "        padding_multiple=16,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-34b-Instruct-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-34b-Instruct-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32000,\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=48,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=22016,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "    # https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"CodeLlama-70b-Instruct-hf\",\n",
    "        hf_config=dict(org=\"codellama\", name=\"CodeLlama-70b-Instruct-hf\"),\n",
    "        block_size=16384,\n",
    "        vocab_size=32016,\n",
    "        padding_multiple=16,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "        rope_base=1000000,\n",
    "    ),\n",
    "]\n",
    "configs.extend(code_llama)\n",
    "\n",
    "\n",
    "########################\n",
    "# garage-bAInd Platypus\n",
    "########################\n",
    "platypus = [\n",
    "    # https://huggingface.co/garage-bAInd/Platypus-30B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Platypus-30B\",\n",
    "        hf_config=dict(org=\"garage-bAInd\", name=\"Platypus-30B\"),\n",
    "        block_size=2048,\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=60,\n",
    "        n_head=52,\n",
    "        n_embd=6656,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-06,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=17920,\n",
    "    ),\n",
    "    # https://huggingface.co/garage-bAInd/Platypus2-7B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Platypus2-7B\",\n",
    "        hf_config=dict(org=\"garage-bAInd\", name=\"Platypus2-7B\"),\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "    ),\n",
    "    # https://huggingface.co/garage-bAInd/Platypus2-13B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Platypus2-13B\",\n",
    "        hf_config=dict(org=\"garage-bAInd\", name=\"Platypus2-13B\"),\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "    # https://huggingface.co/garage-bAInd/Platypus2-70B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Platypus2-70B\",\n",
    "        hf_config=dict(org=\"garage-bAInd\", name=\"Platypus2-70B\"),\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "    ),\n",
    "    # https://huggingface.co/garage-bAInd/Camel-Platypus2-13B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Camel-Platypus2-13B\",\n",
    "        hf_config=dict(org=\"garage-bAInd\", name=\"Camel-Platypus2-13B\"),\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "    # https://huggingface.co/garage-bAInd/Camel-Platypus2-70B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Camel-Platypus2-70B\",\n",
    "        hf_config=dict(org=\"garage-bAInd\", name=\"Camel-Platypus2-70B\"),\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "    ),\n",
    "    # https://huggingface.co/garage-bAInd/Stable-Platypus2-13B/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Stable-Platypus2-13B\",\n",
    "        hf_config=dict(org=\"garage-bAInd\", name=\"Stable-Platypus2-13B\"),\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=40,\n",
    "        n_head=40,\n",
    "        n_embd=5120,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=13824,\n",
    "    ),\n",
    "    # https://huggingface.co/garage-bAInd/Platypus2-70B-instruct/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Platypus2-70B-instruct\",\n",
    "        hf_config=dict(org=\"garage-bAInd\", name=\"Platypus2-70B-instruct\"),\n",
    "        padded_vocab_size=32000,\n",
    "        n_layer=80,\n",
    "        n_head=64,\n",
    "        n_embd=8192,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=28672,\n",
    "    ),\n",
    "]\n",
    "configs.extend(platypus)\n",
    "\n",
    "\n",
    "##################################\n",
    "# togethercomputer LLaMA-2-7B-32K\n",
    "##################################\n",
    "together_llama2_32k = [\n",
    "    # https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"LLaMA-2-7B-32K\",\n",
    "        hf_config=dict(org=\"togethercomputer\", name=\"LLaMA-2-7B-32K\"),\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "        rope_condense_ratio=8,\n",
    "    )\n",
    "]\n",
    "configs.extend(together_llama2_32k)\n",
    "\n",
    "\n",
    "################\n",
    "# Microsoft Phi\n",
    "################\n",
    "phi = [\n",
    "    # https://huggingface.co/microsoft/phi-1_5/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"phi-1_5\",\n",
    "        hf_config=dict(org=\"microsoft\", name=\"phi-1_5\"),\n",
    "        vocab_size=50257,\n",
    "        padded_vocab_size=51200,\n",
    "        block_size=2048,\n",
    "        n_embd=2048,\n",
    "        n_layer=24,\n",
    "        rotary_percentage=0.5,  # 32 / (n_embd / n_head) = 32 / 64\n",
    "        shared_attention_norm=True,\n",
    "        lm_head_bias=True,\n",
    "        gelu_approximate=\"tanh\",\n",
    "    ),\n",
    "    # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"phi-2\",\n",
    "        hf_config=dict(org=\"microsoft\", name=\"phi-2\"),\n",
    "        vocab_size=50257,\n",
    "        padded_vocab_size=51200,\n",
    "        block_size=2048,\n",
    "        n_embd=2560,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=0.4,  # 32 / (n_embd / n_head) = 32 / 80\n",
    "        shared_attention_norm=True,\n",
    "        lm_head_bias=True,\n",
    "        gelu_approximate=\"tanh\",\n",
    "    ),\n",
    "]\n",
    "configs.extend(phi)\n",
    "\n",
    "\n",
    "#############\n",
    "# Mistral AI\n",
    "#############\n",
    "mistral = [\n",
    "    # https://huggingface.co/mistralai/Mistral-7B-v0.1/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Mistral-7B-{}v0.1\",\n",
    "        hf_config=dict(org=\"mistralai\", name=\"Mistral-7B-{}v0.1\"),\n",
    "        padded_vocab_size=32000,\n",
    "        block_size=4096,  # should be 32768 but sliding window attention is not implemented\n",
    "        n_layer=32,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=14336,\n",
    "    ),\n",
    "    # https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Mixtral-8x7B-{}v0.1\",\n",
    "        hf_config=dict(org=\"mistralai\", name=\"Mixtral-8x7B-{}v0.1\"),\n",
    "        padded_vocab_size=32000,\n",
    "        block_size=32768,\n",
    "        n_layer=32,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMoE\",\n",
    "        intermediate_size=14336,\n",
    "        rope_base=1000000,\n",
    "        n_expert=8,\n",
    "        n_expert_per_token=2,\n",
    "    ),\n",
    "]\n",
    "for c in mistral:\n",
    "    for kind in (\"\", \"Instruct-\"):\n",
    "        copy = deepcopy(c)\n",
    "        copy[\"name\"] = c[\"name\"].format(kind)\n",
    "        copy[\"hf_config\"][\"name\"] = c[\"hf_config\"][\"name\"].format(kind)\n",
    "        configs.append(copy)\n",
    "configs.append(\n",
    "    # https://huggingface.co/unsloth/mistral-7b-v0.2/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Mistral-7B-v0.2\",\n",
    "        hf_config=dict(org=\"unsloth\", name=\"Mistral-7B-v0.2\"),\n",
    "        padded_vocab_size=32000,\n",
    "        block_size=32768,\n",
    "        n_layer=32,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=14336,\n",
    "    )\n",
    ")\n",
    "configs.append(\n",
    "    # https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Mistral-7B-Instruct-v0.2\",\n",
    "        hf_config=dict(org=\"mistralai\", name=\"Mistral-7B-Instruct-v0.2\"),\n",
    "        padded_vocab_size=32000,\n",
    "        block_size=32768,\n",
    "        n_layer=32,\n",
    "        n_query_groups=8,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        norm_eps=1e-05,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=14336,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "############\n",
    "# TinyLlama\n",
    "############\n",
    "tiny_llama = [\n",
    "    dict(\n",
    "        name=\"tiny-llama-1.1b{}\",\n",
    "        hf_config=dict(org=\"TinyLlama\", name=\"TinyLlama-1.1B{}\"),\n",
    "        block_size=2048,\n",
    "        vocab_size=32000,\n",
    "        padding_multiple=64,\n",
    "        n_layer=22,\n",
    "        n_head=32,\n",
    "        n_embd=2048,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",  # original TinyLlama uses FusedRMSNorm\n",
    "        norm_eps=1e-5,\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=5632,\n",
    "        n_query_groups=4,\n",
    "    )\n",
    "]\n",
    "for c in tiny_llama:\n",
    "    for kind, hf_postfix in ((\"\", \"-intermediate-step-1431k-3T\"), (\"-chat\", \"-Chat-v1.0\")):\n",
    "        copy = deepcopy(c)\n",
    "        copy[\"name\"] = c[\"name\"].format(kind)\n",
    "        copy[\"hf_config\"][\"name\"] = c[\"hf_config\"][\"name\"].format(hf_postfix)\n",
    "        configs.append(copy)\n",
    "\n",
    "\n",
    "##########################\n",
    "# Trelis Function Calling\n",
    "##########################\n",
    "llama_2_function_calling = [\n",
    "    # https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2/blob/main/config.json\n",
    "    dict(\n",
    "        name=\"Llama-2-7b-chat-hf-function-calling-v2\",\n",
    "        hf_config=dict(org=\"Trelis\", name=\"Llama-2-7b-chat-hf-function-calling-v2\"),\n",
    "        padding_multiple=64,\n",
    "        n_layer=32,\n",
    "        rotary_percentage=1.0,\n",
    "        parallel_residual=False,\n",
    "        bias=False,\n",
    "        norm_class_name=\"RMSNorm\",\n",
    "        mlp_class_name=\"LLaMAMLP\",\n",
    "        intermediate_size=11008,\n",
    "        norm_eps=1e-6,\n",
    "        block_size=4096,\n",
    "        vocab_size=32000,\n",
    "        n_head=32,\n",
    "        n_embd=4096,\n",
    "        rope_base=10000,\n",
    "    )\n",
    "]\n",
    "\n",
    "configs.extend(llama_2_function_calling)\n",
    "\n",
    "name_to_config = {config[\"name\"]: config for config in configs}\n",
    "\n",
    "Config.from_name(\"tiny-llama-1.1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(name='tiny-llama-1.1b', hf_config={'org': 'TinyLlama', 'name': 'TinyLlama-1.1B-intermediate-step-1431k-3T'}, scale_embeddings=False, block_size=2048, vocab_size=32000, padding_multiple=64, padded_vocab_size=32000, n_layer=22, n_head=32, head_size=64, n_embd=2048, rotary_percentage=1.0, parallel_residual=False, bias=False, lm_head_bias=False, n_query_groups=4, shared_attention_norm=False, norm_class_name='RMSNorm', norm_eps=1e-05, mlp_class_name='LLaMAMLP', gelu_approximate='none', intermediate_size=5632, rope_condense_ratio=1, rope_base=10000, n_expert=0, n_expert_per_token=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.from_name(\"tiny-llama-1.1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(name='tiny-llama-1.1b', hf_config={'org': 'TinyLlama', 'name': 'TinyLlama-1.1B-intermediate-step-1431k-3T'}, scale_embeddings=False, block_size=2048, vocab_size=32000, padding_multiple=64, padded_vocab_size=32000, n_layer=22, n_head=32, head_size=64, n_embd=2048, rotary_percentage=1.0, parallel_residual=False, bias=False, lm_head_bias=False, n_query_groups=4, shared_attention_norm=False, norm_class_name='RMSNorm', norm_eps=1e-05, mlp_class_name='LLaMAMLP', gelu_approximate='none', intermediate_size=5632, rope_condense_ratio=1, rope_base=10000, n_expert=0, n_expert_per_token=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.from_name(\"TinyLlama-1.1B-intermediate-step-1431k-3T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'invalid-model-name' is not a supported config name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 99\u001b[0m, in \u001b[0;36mConfig.from_name\u001b[0;34m(cls, name, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     conf_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# get name from an org/model string like \"meta-llama/Meta-Llama-3-8B\"\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 104\u001b[0m, in \u001b[0;36mConfig.from_name\u001b[0;34m(cls, name, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     conf_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minvalid-model-name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 106\u001b[0m, in \u001b[0;36mConfig.from_name\u001b[0;34m(cls, name, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m             conf_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(config \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configs \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_config\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m name)\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a supported config name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m      conf_dict \u001b[38;5;241m=\u001b[39m name_to_config[name]\n",
      "\u001b[0;31mValueError\u001b[0m: 'invalid-model-name' is not a supported config name"
     ]
    }
   ],
   "source": [
    "Config.from_name(\"invalid-model-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(name='tiny-llama-1.1b', hf_config={'org': 'TinyLlama', 'name': 'TinyLlama-1.1B-intermediate-step-1431k-3T'}, scale_embeddings=False, block_size=2048, vocab_size=32000, padding_multiple=64, padded_vocab_size=32000, n_layer=22, n_head=32, head_size=64, n_embd=2048, rotary_percentage=1.0, parallel_residual=False, bias=False, lm_head_bias=False, n_query_groups=4, shared_attention_norm=False, norm_class_name='RMSNorm', norm_eps=1e-05, mlp_class_name='LLaMAMLP', gelu_approximate='none', intermediate_size=5632, rope_condense_ratio=1, rope_base=10000, n_expert=0, n_expert_per_token=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.from_name(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
